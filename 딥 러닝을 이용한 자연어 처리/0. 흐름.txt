[텍스트 전처리]
텍스트 전처리 과정에서 토큰화를 통해 corpus를 token단위로 나눌 수 있는데, 기준이 단어면 word tokenization, 문장이면 sentence segmentation이라고 한다.
 NLTK의 word_tokenize와 WordPunctTokenizer, Keras의 text_to_word_sequence를 통해 word_tokenize처리가 가능하지만 각기 구두점, 아포스트로피 등 처리 규칙이 약간씩 다르다.
표준으로 사용되는 토큰화 방법중 하나인 Penn Treebank Tokenization는 nltk.tokenize에서 제공해주고 있다.
 sentence tokenization역시 nltk.tokenize의 sent_tokenize로 사용이 가능하다. 한국어 sentence segmentation의 경우 KSS(Korean Sentence Splitter)가 주로 사용된다.
한국어의 경우 교착어 특성 상 불규칠 활용, 규칙활용, 자립형태소, 의존형태소 등 고려해야 할 것이 많다. 고로 품사 태깅(Part-of-speech tagging)의 경우도 영어의 경우
nltk.tag의 pos_tag를 이용하여 Penn Treebank POS Tags기준을 사용할 수 있지만, 한국어의 경우 별도로 KoNLPy라는 파이썬 패키지를 사용하여 Okt(Open Korea Text), Mecab, Komoran, Hannanum, KKma
등의 형태소 분석기를 따로 사용해야 한다.
 위의 토큰화 작업 전,후로 노이즈데이터를 제거하는 정제(Cleaning) 및 표현만 다른 단어로 통합시키는 정규화(Normalization)가 함께하는데, 이과정에서 정규식이 잘 사용된다.
정규화(Normalization)의 기법 중 단어의 개수를 줄이기 위해 뿌리 사전단어를 찾아가는 표제어 추출(lemmatization)과 어간을 어림짐작하는 어간추출(stemming)이 있다.
 노이즈 데이터를 제거하는 정제(Cleaning)을 위해선 큰 의미가 없는 단어를 제거하는 작업도 필요한데 이를 특별히 불용어(stopword)라고 부르며 영어의 경우 nltk.corpus의 stopwords에서 확인이 가능하다.
 텍스트 전처리에 사용되는 유용한 도구인 regular expression. re를 알아본다.
 Integer Encoding은 각종 정제, 정규화 과정과 함께 연산효율을 높이기 위해 사용하는 것으로, 보통 단어 등장 빈도수를 기준으로 index를 부여하여 mapping하며, mapping standard(매핑기준)에 해당하지 않는 항목에 대해선 Out-Of-Vocabulary로 따로 indexing한다.
이때 등장 빈도수를 직접 알아낼 수도 있지만, collections.Counter, FreqDist, enumerate, keras의 Tokenizer등을 사용하여 알아내는 편리한 방법도 있다.(feat. most_common, sum(~,[]), np.hstack())
 특정값으로 shape를 조정하는 것이 padding이며, keras의 pad_sequences를 사용하여 편리하게 padding이 가능하다.
 문자를 숫자로 변환하는 Integer Encoding에서 더 구체적으로 계산에 편리한 벡터로 바꾸는 것이 One-Hot Encoding이며, keras의 to_categorial로 변환이 가능하다. 다만 단어간의 연관성을 표현못한다는 단점이 있어 이를 보완하기 위해 다차원 벡터로 나타내는 Word2Vec등의 방법이 존재한다.
 Supervised Learning은 기본적으로 데이터를 학습을 위해 X_train, y_train, X_test, y_test 4가지의 데이터로 분리하는데, 이때 slice를 사용할 수도 있지만, sklean의 train_test_split을 이용하여 특정 개수 혹은 비율로 분리가 가능하다.
 유용한 한국어 전처리 패키지로 PyKoSpacing(오토 띄어쓰기), Py-Hanspell(맞춤법검사), soynlp(cohsion probability & branching entropy로 단어점수표 이용 토크나이저를 지원, L tokenizer로 어절분리가능, normalizer로 반복문자정제가능), ckonlpy(customized konlpy_사용자 단어 추가가능)이 있다.

[언어 모델]
자연스러운 단어 시퀀스를 찾아내는 모델로 적절성은 Machine Translation, Spell Correction, Speech Recognition을 이용하여 판단하며, 내부적으로 conditional probability를 이용한다.
Count기반인 Statistical Language Model의 희소문제를 해결하기 위해 count확률을 높이는 N-gram Language Model이 있지만, 근본적인 문제 해결의 한계와 n의 크기에 따른 trade-off로 인해 Neural Netword Based Language Model이 선호된다.
이때 두 모델의 성능을 비교하기 위해 Perplexity(PPL)을 사용하는데, 값이 낮을수록 좋으며 측정 시점에 모델이 고민중인 선택지의 개수를 brancing factor라고 한다.

[카운트 기반의 단어]
크게 Local(Discrete) Representation_해당단어만집중, Distributed(Continuous) Representation_앞뒤단어도 참고로 나뉘며, Local Representation중 단어순서와 상관없이 frequency만 집중하는 Bag-of-Words(BoW)가 있다.
이는 sklearn의 CountVectorizer에서 지원하며(띄어쓰기 기준이기에 영어만. 한국어 지X) BoW는 frequency만 보아 단어의 중요도를 판단하겠다는 거라 불용어제거하면 성능이 좋아지며 CountVectorizer의 생성자인수 stop_words에 지정하여 불용어를 설정한다.
 서로 다른 문서들의 BoW를 비교할 수 있게 결합한 것이 Document Term Matrix이며, 대부분값이 0인 벡터로 인한 sparse representation과 빈도수 기반 접근이기에 the와같이 의미없는 단어에 가중치를 준다는 문제를 해결하기 위해
DTM에 TF(d,t)_문서d에서의 단어t의 등장횟수 - IDF(d,t)_df(t)_단어t가 등장한 문서수에 반비례하는 수, 즉 TF-IDF가중치를 추가하여 나타낸다. TF-IDF는 모든 문서에서 자주등장하면 중요도를 낮게, 특정 문서에서 자주 등장하면 중요도를 높게 판단한다.
직접 구현하게되면 로그 특성으로 가중치가 0이되는 문제가 발생할 수 있기에, 조정된 식을 이용하여 TF-IDF를 자동계산해주는 사이킷런의 TfidfVectorizer가 있다.

[벡터의 유사도]
일반적으로 비교에 사용하는 문서의 크기는 다르기에 만약 빈도수 기반으로 유사도를 뽑으면(유클리드라더나) 단순히 문서의 길이가 길어서 짧은 문서보다 단어의 반복이 많은것인데도 가중치를 높게 잡을 수 있다.
이러한 문제를 해결하기 위해 벡터의 스칼라가 아닌 방향으로 비교하는 cosin similarity로 해결이 가능하다. sklearn에서 TfidVectorizer로 TF-IDF를 만든다음에 이를 기반으로 cosine_similartity를 수행하면 된다. 복습차원에서 TF-IDF는 빈도수 기반의 단어의 가중치(중요도)정보이다.
  그외 유사도 기법으로 다차원 두점사이 거리를 구하는 유클리드 유사도, 합집합중 교집합의 비율을 구하는 자카드 유사도, 편집거리(같아지려면 몇번을 수정해야하는지)를 구하는 레벤슈타인 알고리즘이 있다.

[머신러닝]
Validation data의 목적은 과적합여부판단, 하이퍼파라미터의 조정이다. Classification은 Binary, Multi-class, Multi-table로 나뉘며 Regression은 Linear, Logistic이 있다. 머신러닝은 Supervised, Unsupervised, Self-Supervised가 있다. 
Accuracy의 세부적인 정보를 위한 것이 Confusion Matrix이며, Precision(예측T중 실제T), Recall(실제T중 예측T), Accuracy(전체중 정확히 판단한거)로 성능을 판단한다. 훈련데이터 손실이 증가하면 과적합 징후이다.
 Binary Classification을 해결하기 위해 Logistic Regression이 사용되며, data와 label을 그래프로 표현하면 특정 시점을 기준으로 0에서 1로 변경되며, 대표적인 이러한 형태의 그래프로 Sigmoid function이 있다. 
Binary Classification의 loss function의 Local Minimum문제를 해결하기 위해 로그를 사용한 새로운 objective function을 도입하였고 이가 Cross Entropy function이다.
 케라스로 구현 시 Sequential()로 모델을 만든 후, add를 통해 Dense의 매개변수로 activation과 output_dim, input_dim을 전해주면 된다. optimizer을 instantiation한 뒤 model compile시 optimizer, loss function, metrics를 지정후 fit으로 training하면 된다.
다중 입력의 경우에는 model add시 Dence의 parameter로 input_dim을 조정하면 된다. 여기서 다중 선형회귀(avtivation=linear)와 다중 로지스틱 회귀(activation=sigmoid)를 비교하는데, 둘 다 Binary Classification에 이용되지만 그 분류 기준이 직선이냐, 곡선이냐 차이이다. 또한
선형회귀의 경우 확률값이 1을 초과하거나 0 미만이 될 수 있지만, 이를 확률적으로 표현시키기 위해 0~1사이에 분포하는 시그모이드함수를 이용하여 확률 개념으로 문제를 접근하는 것이다.
 벡터와 행렬 연산에서 텐서의 차원은 Acix의 개수로 규정하며, 다차원 행렬을 3차원 텐서로 부르며 이때부터 본격적인 텐서로 부른다. 이때 3D Tensor을 Cube로 부르기도 한다. (텐서의 배치크기 동시지정 input_shape->batch_input_shape)
벡터의 dot product(inner product)는 차원이 같아야 하며, 앞벡터가 행벡터(가로벡터), 뒷벳터가 열벡터(세로벡터)여야 한다 (결과는 스칼라). 행렬의 곱셈의 조건은 A의 열, B의 행이 같아야 하며, 결과는 A의 행, B의 열을 따른다. 이 규칙을 이용하여 입력과 출력의 행렬 크기로부터 가중치 행렬W과 편향 행렬B의 크기를 추론할 수 있다.
 Multi-class Classification문제에서는 softmax함수를 주로 사용하며, 비용은 로지스틱 회귀의 비용함수와 같은 CrossEntropy(one-hot encoded data를 이용하기에 categorical_crossentropy사용)함수를 사용한다. seaboen모듈을 이용하여 datafram을 plot가시화할 수 있다.

[딥러닝]
다수의 입력이 가중치와 곱해진 합이 threshold(임계치_세타)를 넘으면 1을 출력하는 초기형태의 인공신경망을 Perceptron이라 부르며, Step function이라고도 한다. 뉴런의 출력을 변경시키는 함수를 activation function이라 하며 sigmoid, softmax등이 있다.
0과1로 하나를 출력하는 회로인 Gate를 Single-Layer Perceptron으로 볼 수 있고 이들을 조합하여 MultiLayer Perceptron을 만들 수 있다. 이때 층을 쌓으며 입출력층 사이의 층을 hidden layer라고 하며, 이러한 은닉층이 2개이상인 신경망을 Deep Neural Network라고 부르며, 이를 학습시키는 것이 Deep Learning이다.
 인공신경망의 기본내용으로 Feed-Forward Neural Network(FFNN)은 순방향연결, Fully-connected layer(Dense layer)는 모든 뉴런이 이전 뉴런과 연결된것, Activation Function은 뉴런의 출력값을 결정하는 것으로 비선형함수이다.(선형이라면 (은닉)층을 쌓을 수 없기에)
이때 새로운 가중치를 포함시킬 목적으로 추가되는 linear layer의 경우 nonlinear layer과 비교하기 위해 선형층, 투사층이라고도 부른다. 주로 사용되는 activation function은 Step function, Sigmoid function->Hyperbolic tangent function, ReLU->Leaky ReLU, Softmax function이 있다.
 가중치와 편향 업데이터(backpropagation)을 위해서는 먼저 weight와 bias의 shape를 알아야하는데, 이는 행렬의 관점에서 행렬곱의 규칙을 통해 추론이 가능하다. 이 weight와 bias의 개수가 해당 모델의 parameter개수이고, model.summary()를 통해 확인할 수 있다.
손실함수로는 연속형변수를 위한 MSE, Binary Classification(sigmoid)를 위한 binary_crossentropy, Multi-Class Classification(softmax)를 위한 CategoricalCrossentropy, Interger encoded를 위한 sparse_categorical_crossentropy가 있다.
배치 크기로는 전체를 하나의 배치로 두는 Batch Gradient Descent, 배치크기를 1로 두는 Stochastic Gradient Descent, 배치크기를 지정하는 Mini-Batch Gradient Descent가 있다.
Optimizer로는 관성을 주는 Momentum, 매개변수별 다른 학습률의 Adagrad, Adagrad의 지나치게 낮은 학습률부분의 수식을 변경한 RMSprop, RMSprop에 Momentum을 더한 Adam이 있으며, 대부분 model.compile의 optimzier인자에 문자열로 전달이 가능하다.
 과적합을 막기 위해 데이터 양을 늘리거나 parameter을 줄이거나, Weight Regularization(L1 Norm, L2 Norm)을 적용하거나, Dropout을 사용할 수 있다.
 Gradient Vanishing, Exploding을 막기 위해 Leaky ReLU등을 activation function으로 사용하거나, Gradient Clipping을 지정하거나, Xavier Initialization & He Initialization으로 weight initialization으로 해결할 수 있다.
다른 방법으로 변화폭을 정규화 시키는 Batch Normalization이나 Layer Normalization을 사용할 수 있다.
 Sequential API, Functional API, Subclassing API의 방법이 있다. 자세한건 commit 내용을 참고하자.
 texts_to_metrix로 count, binary, freq, tfidf모드 전처리가 가능하다. null인 sample이 있는지 data.isnull().values.any(), 중복을 제외 data['email'].nunique()를 사용할 수 있다.
tokenizer 인스턴스화 시 vocab_size를 정해 frequency기준 최대 단어장크기를 설정할 수 있다.
 기존의 N-gram이 가지고 있던 Sparsity problem을 단어의 의미적 유사성을 반영하여 극복가능한데, 이러한 모델의 초기 모델이 Feed Forward Neural Network Language Model이다.
이는 n-gram처럼 window크기만큼을 고려하며 projection layer을 통해 가중치 행렬(lookup table로 사용)을 곱한뒤 N차원의 embedding vector로 concatenate를 사용하여 매핑한다.
그 뒤 hidden layer을 통해 얻은 벡터를 출력층에서 softmax하여 다음 단어의 확률을 나타낸다.(loss=cross-entropy) 이를 통해 단어가 다르더라도 앞의 단어들이 같게끔 유사한 목적으로 사용된다면 유사한 임베딩 벡터값을 얻는다.
다만 이전의 모든 단어를 고려하지 못한다는 n-gram의 문제를 공유하는데, 이는 Recurrent Neural Network Language Model을 통해 극복 가능하다.

[순환신경망]
 FFNN이 아닌 신경망 중 하나로, 입력벡터와 출력벡터 길이에 따라 구분되는데, Image Captioning에 이용되는 one-to-many, sentiment classification에 사용되는 many-to-one, 개체명 인식등에 사용되는 many-to-many가 있다.
(batch_size, timesteps, input_dim)의 3D Tensor을 입력으로 받으며, return_sequences인자에 따라 (batch_size, timesteps, output_dim)혹은 (batch_size, output_dim)의 Tensor를 리턴한다.
2개이상의 은닉층을 가지는 Deep RNN, forward&backward hidden state를 받응 Bidirectional RNN, 두개가 합쳐진 Bidirectional Deep RNN이 있으며 이는 태깅 작업에 유용하다.
 Valilla RNN의 길이가 길때, 초기의 hidden_state가 영향력을 잃는 The problem of Long-Term Dependencies를 해결하기 위해 cell_state를 추가한 LSTM(Long Short-Term Memory)를 사용하며, 
이는 이전의 cell_state값의 반영도를 결정하는 삭제게이트, 현재 입력을 반영하는 입력 게이트와의 연산후 출력게이트를 통과시켜 현재시점의 hidden_state를 결정한다. LSTM의 구조를 단순화시킨 Gated Recurrent Unit)GRU)가 있는데 성능은 비슷하다.
 LSTM은 (hidden_states, last_hidden_state, last_cell_state)을 반환하며, Bidirectional LSTM은 (hidden_states, forward_h, forward_c, backward_h, backward_c)를 리턴한다. 이때 Bidirectional의 경우 return_state를 이용할 경우 정방향과 역방향의 state가 연결되어 나오는데, 일관성이 없으니 유의하자.

[워드 임베딩(Word Embedding)]
Sparse Representation의 단점을 해소하기 위해 Dense Vector로 표현(distributed representation)하는 것을 Word Embedding이라하며, 
Word2Vec의 기법으로 context word로 centor word를 추론하는CBOW(Continuous Bag Of Word), centor word로 context word를 추론하는 Skip-gram이 있다.
projection layer가 lookup table로서 존재하는데, CBOW의 경우 centor word를 구하기에 투사층에서 입력벡터들이 평균을 구한다. 즉, 구조가 Input_layer->Projection_layer->Output_layer로 간단하기에
hidden layer가 추가적으로 존재하는 NNLM과 비교하여 우위를 가진다.(feat. 좀 더 좋은 기법_hierachical softmax, negative sampling)
 gensim.models.Word2Vec을 사용할때의 hyperparameter들은 vector_size는 embedding된 벡터의 dimention, min_count는 최소 빈도수 제한, sg는 Skip-gram과 CBOW의 방식선택을 의미한다.
사전훈련된 Word2Vec을 google에서 지원해준다.
 네거티브 샘플링은 일반적인 Word2Vec의 학습 후 update시 단어와 관련없는 것까지 업데이트 하는 비효율성을 줄이기기 위한 것으로, 이웃관계인 단어들은 레이블1의 값, 랜덤값은 레이블 0의 값으로 세팅되고
center word, context word별 embedding layer을 통과하고 그 값이 내적과 label을 비교하여 loss를 update한다.
 Glove는 LSA(Letent Semantic Analysis), Word2Vec의 기반인 카운트기반, 예측기반을 모두 사용하여 추론하는데, 일부주변단어만 확인하는 기존의 단점을 Co-occurence Probabiltity를 통해 보완한다. 
FastText는 Word2Vec의 모르는단어가 나오면 추론을 못한다는 단점을 보완하여 n-gram으로 subword를 word embedding하여 오타나 희귀단어에 있어 오류를 던지는 Word2Vec보다 강세를 보인다. Pre-trained word embedding을 Word2Vec혹은 GloVe로 할 수 있다.

 잠시 정리의 필요성을 느껴정리해보자면 Dense vector로 표현하는 것이 Word Embedding이며 CBOW, Skipgram의 Word2Vec은 단점인 불필요한 update를 해소하기 위해 negative sampling을 톨해 window에 있는 주변단어와 중심단어의 내적을 label과 비교하여 update한다.
GloVe는 Word2Vec, LSA에서 window범위내의 단어만 확인하는 것을 Co-occruence Probability를 통해 보완한다. 여기서 더 나아가 FastText는 Word2Vec이 오타와 같이 조금이라도 다른 단어, 유사한 단어에 대한 추론력을 높이기 위해 n-gram(3~6)의 subword개념을 도입하였다. 이 GloVe와 Word2Vec은 pre-trained version을 지원한다.
위에 언급된 Word Embedding기법들을 살펴보면 Word2Vec은 Sparse representation 즉 생소한 단어의 가중치가 높게 측정되는 것을 막았고, Glove는 전체 단어가 추론에 고려되게 하였으며, FastText는 subword에 대한 추론이 가능하게 하였다.
 하지만 위의 Word2Vec, GloVe, FastText가 해결하지 못하는 문제가 하나 있었다.  일부 글자는 같은데 의미가 다른 단어 즉 '탄광' '광원' & 'Bank Account(은행계좌)', 'River Bank(강둑)'과 같은 경우에 둘은 다른 뜻을 가진 단어이지만 같은 글자를 사용한 경우, 동음이의어의 경우 이 둘을 아예 다른단어라고 판단하기 부족하다.
여기서 이 문제를 해결하기 위해 표기에 집중하는 것이 아닌 문맥에 집중하여 임베딩을 한다는 생소한 개념 Contextualized word-embeddings 방법론 , 엘모(Embeddings from Language Model)가 등장한다. 고로 기존의 문맥을 판단하는 지표를 추가적으로 늘리기 위해 Bidirectional Language Model(BILM)을 사용한다.
이는 문맥을 판단하기 위한 데이터(도구)를 추가로 사용하기 위헤 기존에 Forward Language Model 즉, 순방향으로 문맥을 판단하는 것에 그치지 않고 역방향으로도 문맥을 판단하게끔 Forward Language Model과 Backward Language Model을 합쳐 Bidirectional Language Model, biNM을 사용한다.
이때, 말했다시피 문맥을 판단하기 위한 데이터(도구)를 늘리기위해 조금의 규칙성이라도 더 고려하기 위해 Backward Language Model을 사용한 것이기에 기존에 우리가 알고있던 양방향 RNN(Bidirectional RNN)과는 다르다.
기존의 양방향 RNN은 순방향 RNN의 hidden_state와 역방향 RNN의 hidden_state가 연결(concatenate)되어 있었지만, biLM은 Forward Language Model과 Backward Language Model을 아예 분리하여 별개의 모델로 보고 따로 학습한다.(구현은 코드 참고!)
이는 기존의 임베딩 벡터와 함께 사용될 수 있는데, 준비된 ELMO representation(BILM을 지난 embedding vector)와 GloVe같은 기존의 embedding vector와 함께 concatenate하여 사용한다. 호우! 왜 둘을 연결해서 사용하는지는 잘 모르겠지만 일단 이게 이해 최선
