[텍스트 전처리]
텍스트 전처리 과정에서 토큰화를 통해 corpus를 token단위로 나눌 수 있는데, 기준이 단어면 word tokenization, 문장이면 sentence segmentation이라고 한다.
 NLTK의 word_tokenize와 WordPunctTokenizer, Keras의 text_to_word_sequence를 통해 word_tokenize처리가 가능하지만 각기 구두점, 아포스트로피 등 처리 규칙이 약간씩 다르다.
표준으로 사용되는 토큰화 방법중 하나인 Penn Treebank Tokenization는 nltk.tokenize에서 제공해주고 있다.
 sentence tokenization역시 nltk.tokenize의 sent_tokenize로 사용이 가능하다. 한국어 sentence segmentation의 경우 KSS(Korean Sentence Splitter)가 주로 사용된다.
한국어의 경우 교착어 특성 상 불규칠 활용, 규칙활용, 자립형태소, 의존형태소 등 고려해야 할 것이 많다. 고로 품사 태깅(Part-of-speech tagging)의 경우도 영어의 경우
nltk.tag의 pos_tag를 이용하여 Penn Treebank POS Tags기준을 사용할 수 있지만, 한국어의 경우 별도로 KoNLPy라는 파이썬 패키지를 사용하여 Okt(Open Korea Text), Mecab, Komoran, Hannanum, KKma
등의 형태소 분석기를 따로 사용해야 한다.
 위의 토큰화 작업 전,후로 노이즈데이터를 제거하는 정제(Cleaning) 및 표현만 다른 단어로 통합시키는 정규화(Normalization)가 함께하는데, 이과정에서 정규식이 잘 사용된다.
정규화(Normalization)의 기법 중 단어의 개수를 줄이기 위해 뿌리 사전단어를 찾아가는 표제어 추출(lemmatization)과 어간을 어림짐작하는 어간추출(stemming)이 있다.
 노이즈 데이터를 제거하는 정제(Cleaning)을 위해선 큰 의미가 없는 단어를 제거하는 작업도 필요한데 이를 특별히 불용어(stopword)라고 부르며 영어의 경우 nltk.corpus의 stopwords에서 확인이 가능하다.
 텍스트 전처리에 사용되는 유용한 도구인 regular expression. re를 알아본다.
 Integer Encoding은 각종 정제, 정규화 과정과 함께 연산효율을 높이기 위해 사용하는 것으로, 보통 단어 등장 빈도수를 기준으로 index를 부여하여 mapping하며, mapping standard(매핑기준)에 해당하지 않는 항목에 대해선 Out-Of-Vocabulary로 따로 indexing한다.
이때 등장 빈도수를 직접 알아낼 수도 있지만, collections.Counter, FreqDist, enumerate, keras의 Tokenizer등을 사용하여 알아내는 편리한 방법도 있다.(feat. most_common, sum(~,[]), np.hstack())
 특정값으로 shape를 조정하는 것이 padding이며, keras의 pad_sequences를 사용하여 편리하게 padding이 가능하다.
 문자를 숫자로 변환하는 Integer Encoding에서 더 구체적으로 계산에 편리한 벡터로 바꾸는 것이 One-Hot Encoding이며, keras의 to_categorial로 변환이 가능하다. 다만 단어간의 연관성을 표현못한다는 단점이 있어 이를 보완하기 위해 다차원 벡터로 나타내는 Word2Vec등의 방법이 존재한다.
 Supervised Learning은 기본적으로 데이터를 학습을 위해 X_train, y_train, X_test, y_test 4가지의 데이터로 분리하는데, 이때 slice를 사용할 수도 있지만, sklean의 train_test_split을 이용하여 특정 개수 혹은 비율로 분리가 가능하다.
 유용한 한국어 전처리 패키지로 PyKoSpacing(오토 띄어쓰기), Py-Hanspell(맞춤법검사), soynlp(cohsion probability & branching entropy로 단어점수표 이용 토크나이저를 지원, L tokenizer로 어절분리가능, normalizer로 반복문자정제가능), ckonlpy(customized konlpy_사용자 단어 추가가능)이 있다.

[언어 모델]
자연스러운 단어 시퀀스를 찾아내는 모델로 적절성은 Machine Translation, Spell Correction, Speech Recognition을 이용하여 판단하며, 내부적으로 conditional probability를 이용한다.
Count기반인 Statistical Language Model의 희소문제를 해결하기 위해 count확률을 높이는 N-gram Language Model이 있지만, 근본적인 문제 해결의 한계와 n의 크기에 따른 trade-off로 인해 Neural Netword Based Language Model이 선호된다.
이때 두 모델의 성능을 비교하기 위해 Perplexity(PPL)을 사용하는데, 값이 낮을수록 좋으며 측정 시점에 모델이 고민중인 선택지의 개수를 brancing factor라고 한다.

[카운트 기반의 단어]
크게 Local(Discrete) Representation_해당단어만집중, Distributed(Continuous) Representation_앞뒤단어도 참고로 나뉘며, Local Representation중 단어순서와 상관없이 frequency만 집중하는 Bag-of-Words(BoW)가 있다.
이는 sklearn의 CountVectorizer에서 지원하며(띄어쓰기 기준이기에 영어만. 한국어 지원X) BoW는 frequency만 보아 단어의 중요도를 판단하겠다는 거라 불용어제거하면 성능이 좋아지며 CountVectorizer의 생성자인수 stop_words에 지정하여 불용어를 설정한다.
 서로 다른 문서들의 BoW를 비교할 수 있게 결합한 것이 Document Term Matrix이며, 대부분값이 0인 벡터로 인한 sparse representation과 빈도수 기반 접근이기에 the와같이 의미없는 단어에 가중치를 준다는 문제를 해결하기 위해
DTM에 TF(d,t)_문서d에서의 단어t의 등장횟수 - IDF(d,t)_df(t)_단어t가 등장한 문서수에 반비례하는 수, 즉 TF-IDF가중치를 추가하여 나타낸다. TF-IDF는 모든 문서에서 자주등장하면 중요도를 낮게, 특정 문서에서 자주 등장하면 중요도를 높게 판단한다.
직접 구현하게되면 로그 특성으로 가중치가 0이되는 문제가 발생할 수 있기에, 조정된 식을 이용하여 TF-IDF를 자동계산해주는 사이킷런의 TfidfVectorizer가 있다.

[벡터의 유사도]
일반적으로 비교에 사용하는 문서의 크기는 다르기에 만약 빈도수 기반으로 유사도를 뽑으면(유클리드라더나) 단순히 문서의 길이가 길어서 짧은 문서보다 단어의 반복이 많은것인데도 가중치를 높게 잡을 수 있다.
이러한 문제를 해결하기 위해 벡터의 스칼라가 아닌 방향으로 비교하는 cosin similarity로 해결이 가능하다. sklearn에서 TfidVectorizer로 TF-IDF를 만든다음에 이를 기반으로 cosine_similartity를 수행하면 된다. 복습차원에서 TF-IDF는 빈도수 기반의 단어의 가중치(중요도)정보이다.
  그외 유사도 기법으로 다차원 두점사이 거리를 구하는 유클리드 유사도, 합집합중 교집합의 비율을 구하는 자카드 유사도, 편집거리(같아지려면 몇번을 수정해야하는지)를 구하는 레벤슈타인 알고리즘이 있다.

[머신러닝]
Validation data의 목적은 과적합여부판단, 하이퍼파라미터의 조정이다. Classification은 Binary, Multi-class, Multi-table로 나뉘며 Regression은 Linear, Logistic이 있다. 머신러닝은 Supervised, Unsupervised, Self-Supervised가 있다. 
Accuracy의 세부적인 정보를 위한 것이 Confusion Matrix이며, Precision(예측T중 실제T), Recall(실제T중 예측T), Accuracy(전체중 정확히 판단한거)로 성능을 판단한다. 훈련데이터 손실이 증가하면 과적합 징후이다.
 Binary Classification을 해결하기 위해 Logistic Regression이 사용되며, data와 label을 그래프로 표현하면 특정 시점을 기준으로 0에서 1로 변경되며, 대표적인 이러한 형태의 그래프로 Sigmoid function이 있다. 
Binary Classification의 loss function의 Local Minimum문제를 해결하기 위해 로그를 사용한 새로운 objective function을 도입하였고 이가 Cross Entropy function이다.
 케라스로 구현 시 Sequential()로 모델을 만든 후, add를 통해 Dense의 매개변수로 activation과 output_dim, input_dim을 전해주면 된다. optimizer을 instantiation한 뒤 model compile시 optimizer, loss function, metrics를 지정후 fit으로 training하면 된다.
다중 입력의 경우에는 model add시 Dence의 parameter로 input_dim을 조정하면 된다. 여기서 다중 선형회귀(avtivation=linear)와 다중 로지스틱 회귀(activation=sigmoid)를 비교하는데, 둘 다 Binary Classification에 이용되지만 그 분류 기준이 직선이냐, 곡선이냐 차이이다. 또한
선형회귀의 경우 확률값이 1을 초과하거나 0 미만이 될 수 있지만, 이를 확률적으로 표현시키기 위해 0~1사이에 분포하는 시그모이드함수를 이용하여 확률 개념으로 문제를 접근하는 것이다.
 벡터와 행렬 연산에서 텐서의 차원은 Acix의 개수로 규정하며, 다차원 행렬을 3차원 텐서로 부르며 이때부터 본격적인 텐서로 부른다. 이때 3D Tensor을 Cube로 부르기도 한다. (텐서의 배치크기 동시지정 input_shape->batch_input_shape)
벡터의 dot product(inner product)는 차원이 같아야 하며, 앞벡터가 행벡터(가로벡터), 뒷벳터가 열벡터(세로벡터)여야 한다 (결과는 스칼라). 행렬의 곱셈의 조건은 A의 열, B의 행이 같아야 하며, 결과는 A의 행, B의 열을 따른다. 이 규칙을 이용하여 입력과 출력의 행렬 크기로부터 가중치 행렬W과 편향 행렬B의 크기를 추론할 수 있다.
 Multi-class Classification문제에서는 softmax함수를 주로 사용하며, 비용은 로지스틱 회귀의 비용함수와 같은 CrossEntropy(one-hot encoded data를 이용하기에 categorical_crossentropy사용)함수를 사용한다. seaboen모듈을 이용하여 datafram을 plot가시화할 수 있다.

[딥러닝]
다수의 입력이 가중치와 곱해진 합이 threshold(임계치_세타)를 넘으면 1을 출력하는 초기형태의 인공신경망을 Perceptron이라 부르며, Step function이라고도 한다. 뉴런의 출력을 변경시키는 함수를 activation function이라 하며 sigmoid, softmax등이 있다.
0과1로 하나를 출력하는 회로인 Gate를 Single-Layer Perceptron으로 볼 수 있고 이들을 조합하여 MultiLayer Perceptron을 만들 수 있다. 이때 층을 쌓으며 입출력층 사이의 층을 hidden layer라고 하며, 이러한 은닉층이 2개이상인 신경망을 Deep Neural Network라고 부르며, 이를 학습시키는 것이 Deep Learning이다.
 인공신경망의 기본내용으로 Feed-Forward Neural Network(FFNN)은 순방향연결, Fully-connected layer(Dense layer)는 모든 뉴런이 이전 뉴런과 연결된것, Activation Function은 뉴런의 출력값을 결정하는 것으로 비선형함수이다.(선형이라면 (은닉)층을 쌓을 수 없기에)
이때 새로운 가중치를 포함시킬 목적으로 추가되는 linear layer의 경우 nonlinear layer과 비교하기 위해 선형층, 투사층이라고도 부른다. 주로 사용되는 activation function은 Step function, Sigmoid function->Hyperbolic tangent function, ReLU->Leaky ReLU, Softmax function이 있다.
 가중치와 편향 업데이터(backpropagation)을 위해서는 먼저 weight와 bias의 shape를 알아야하는데, 이는 행렬의 관점에서 행렬곱의 규칙을 통해 추론이 가능하다. 이 weight와 bias의 개수가 해당 모델의 parameter개수이고, model.summary()를 통해 확인할 수 있다.
손실함수로는 연속형변수를 위한 MSE, Binary Classification(sigmoid)를 위한 binary_crossentropy, Multi-Class Classification(softmax)를 위한 CategoricalCrossentropy, Interger encoded를 위한 sparse_categorical_crossentropy가 있다.
배치 크기로는 전체를 하나의 배치로 두는 Batch Gradient Descent, 배치크기를 1로 두는 Stochastic Gradient Descent, 배치크기를 지정하는 Mini-Batch Gradient Descent가 있다.
Optimizer로는 관성을 주는 Momentum, 매개변수별 다른 학습률의 Adagrad, Adagrad의 지나치게 낮은 학습률부분의 수식을 변경한 RMSprop, RMSprop에 Momentum을 더한 Adam이 있으며, 대부분 model.compile의 optimzier인자에 문자열로 전달이 가능하다.
 과적합을 막기 위해 데이터 양을 늘리거나 parameter을 줄이거나, Weight Regularization(L1 Norm, L2 Norm)을 적용하거나, Dropout을 사용할 수 있다.
 Gradient Vanishing, Exploding을 막기 위해 Leaky ReLU등을 activation function으로 사용하거나, Gradient Clipping을 지정하거나, Xavier Initialization & He Initialization으로 weight initialization으로 해결할 수 있다.
다른 방법으로 변화폭을 정규화 시키는 Batch Normalization이나 Layer Normalization을 사용할 수 있다.
 Sequential API, Functional API, Subclassing API의 방법이 있다. 자세한건 commit 내용을 참고하자.
 texts_to_metrix로 count, binary, freq, tfidf모드 전처리가 가능하다. null인 sample이 있는지 data.isnull().values.any(), 중복을 제외 data['email'].nunique()를 사용할 수 있다.
tokenizer 인스턴스화 시 vocab_size를 정해 frequency기준 최대 단어장크기를 설정할 수 있다.
 기존의 N-gram이 가지고 있던 Sparsity problem을 단어의 의미적 유사성을 반영하여 극복가능한데, 이러한 모델의 초기 모델이 Feed Forward Neural Network Language Model이다.
이는 n-gram처럼 window크기만큼을 고려하며 projection layer을 통해 가중치 행렬(lookup table로 사용)을 곱한뒤 N차원의 embedding vector로 concatenate를 사용하여 매핑한다.
그 뒤 hidden layer을 통해 얻은 벡터를 출력층에서 softmax하여 다음 단어의 확률을 나타낸다.(loss=cross-entropy) 이를 통해 단어가 다르더라도 앞의 단어들이 같게끔 유사한 목적으로 사용된다면 유사한 임베딩 벡터값을 얻는다.
다만 이전의 모든 단어를 고려하지 못한다는 n-gram의 문제를 공유하는데, 이는 Recurrent Neural Network Language Model을 통해 극복 가능하다.

[순환신경망]
 FFNN이 아닌 신경망 중 하나로, 입력벡터와 출력벡터 길이에 따라 구분되는데, Image Captioning에 이용되는 one-to-many, sentiment classification에 사용되는 many-to-one, 개체명 인식등에 사용되는 many-to-many가 있다.
(batch_size, timesteps, input_dim)의 3D Tensor을 입력으로 받으며, return_sequences인자에 따라 (batch_size, timesteps, output_dim)혹은 (batch_size, output_dim)의 Tensor를 리턴한다.
2개이상의 은닉층을 가지는 Deep RNN, forward&backward hidden state를 받응 Bidirectional RNN, 두개가 합쳐진 Bidirectional Deep RNN이 있으며 이는 태깅 작업에 유용하다.
 Valilla RNN의 길이가 길때, 초기의 hidden_state가 영향력을 잃는 The problem of Long-Term Dependencies를 해결하기 위해 cell_state를 추가한 LSTM(Long Short-Term Memory)를 사용하며, 
이는 이전의 cell_state값의 반영도를 결정하는 삭제게이트, 현재 입력을 반영하는 입력 게이트와의 연산후 출력게이트를 통과시켜 현재시점의 hidden_state를 결정한다. LSTM의 구조를 단순화시킨 Gated Recurrent Unit)GRU)가 있는데 성능은 비슷하다.
 LSTM은 (hidden_states, last_hidden_state, last_cell_state)을 반환하며, Bidirectional LSTM은 (hidden_states, forward_h, forward_c, backward_h, backward_c)를 리턴한다. 이때 Bidirectional의 경우 return_state를 이용할 경우 정방향과 역방향의 state가 연결되어 나오는데, 일관성이 없으니 유의하자.

[워드 임베딩(Word Embedding)]
Sparse Representation의 단점을 해소하기 위해 Dense Vector로 표현(distributed representation)하는 것을 Word Embedding이라하며, 
Word2Vec의 기법으로 context word로 centor word를 추론하는CBOW(Continuous Bag Of Word), centor word로 context word를 추론하는 Skip-gram이 있다.
projection layer가 lookup table로서 존재하는데, CBOW의 경우 centor word를 구하기에 투사층에서 입력벡터들이 평균을 구한다. 즉, 구조가 Input_layer->Projection_layer->Output_layer로 간단하기에
hidden layer가 추가적으로 존재하는 NNLM과 비교하여 우위를 가진다.(feat. 좀 더 좋은 기법_hierachical softmax, negative sampling)
 gensim.models.Word2Vec을 사용할때의 hyperparameter들은 vector_size는 embedding된 벡터의 dimention, min_count는 최소 빈도수 제한, sg는 Skip-gram과 CBOW의 방식선택을 의미한다.
사전훈련된 Word2Vec을 google에서 지원해준다.
 네거티브 샘플링은 일반적인 Word2Vec의 학습 후 update시 단어와 관련없는 것까지 업데이트 하는 비효율성을 줄이기기 위한 것으로, 이웃관계인 단어들은 레이블1의 값, 랜덤값은 레이블 0의 값으로 세팅되고
center word, context word별 embedding layer을 통과하고 그 값이 내적과 label을 비교하여 loss를 update한다.
 Glove는 LSA(Letent Semantic Analysis), Word2Vec의 기반인 카운트기반, 예측기반을 모두 사용하여 추론하는데, 일부주변단어만 확인하는 기존의 단점을 Co-occurence Probabiltity를 통해 보완한다. 
FastText는 Word2Vec의 모르는단어가 나오면 추론을 못한다는 단점을 보완하여 n-gram으로 subword를 word embedding하여 오타나 희귀단어에 있어 오류를 던지는 Word2Vec보다 강세를 보인다. Pre-trained word embedding을 Word2Vec혹은 GloVe로 할 수 있다.

 잠시 정리의 필요성을 느껴정리해보자면 Dense vector로 표현하는 것이 Word Embedding이며 CBOW, Skipgram의 Word2Vec은 단점인 불필요한 update를 해소하기 위해 negative sampling을 톨해 window에 있는 주변단어와 중심단어의 내적을 label과 비교하여 update한다.
GloVe는 Word2Vec, LSA에서 window범위내의 단어만 확인하는 것을 Co-occruence Probability를 통해 보완한다. 여기서 더 나아가 FastText는 Word2Vec이 오타와 같이 조금이라도 다른 단어, 유사한 단어에 대한 추론력을 높이기 위해 n-gram(3~6)의 subword개념을 도입하였다. 이 GloVe와 Word2Vec은 pre-trained version을 지원한다.
위에 언급된 Word Embedding기법들을 살펴보면 Word2Vec은 Sparse representation 즉 생소한 단어의 가중치가 높게 측정되는 것을 막았고, Glove는 전체 단어가 추론에 고려되게 하였으며, FastText는 subword에 대한 추론이 가능하게 하였다.
 하지만 위의 Word2Vec, GloVe, FastText가 해결하지 못하는 문제가 하나 있었다.  일부 글자는 같은데 의미가 다른 단어 즉 '탄광' '광원' & 'Bank Account(은행계좌)', 'River Bank(강둑)'과 같은 경우에 둘은 다른 뜻을 가진 단어이지만 같은 글자를 사용한 경우, 동음이의어의 경우 이 둘을 아예 다른단어라고 판단하기 부족하다.
여기서 이 문제를 해결하기 위해 표기에 집중하는 것이 아닌 문맥에 집중하여 임베딩을 한다는 생소한 개념 Contextualized word-embeddings 방법론 , 엘모(Embeddings from Language Model)가 등장한다. 고로 기존의 문맥을 판단하는 지표를 추가적으로 늘리기 위해 Bidirectional Language Model(BILM)을 사용한다.
이는 문맥을 판단하기 위한 데이터(도구)를 추가로 사용하기 위헤 기존에 Forward Language Model 즉, 순방향으로 문맥을 판단하는 것에 그치지 않고 역방향으로도 문맥을 판단하게끔 Forward Language Model과 Backward Language Model을 합쳐 Bidirectional Language Model, biNM을 사용한다.
이때, 말했다시피 문맥을 판단하기 위한 데이터(도구)를 늘리기위해 조금의 규칙성이라도 더 고려하기 위해 Backward Language Model을 사용한 것이기에 기존에 우리가 알고있던 양방향 RNN(Bidirectional RNN)과는 다르다.
기존의 양방향 RNN은 순방향 RNN의 hidden_state와 역방향 RNN의 hidden_state가 연결(concatenate)되어 있었지만, biLM은 Forward Language Model과 Backward Language Model을 아예 분리하여 별개의 모델로 보고 따로 학습한다.(구현은 코드 참고!)
이는 기존의 임베딩 벡터와 함께 사용될 수 있는데, 준비된 ELMO representation(BILM을 지난 embedding vector)와 GloVe같은 기존의 embedding vector와 함께 concatenate하여 사용한다. 호우! 왜 둘을 연결해서 사용하는지는 잘 모르겠지만 일단 이게 이해 최선

 문서 벡터를 이용하여 문자간의 비교를 통한 추천 시스템 제작이 가능한데, 문서의 벡터들의 평균을 코사인 유사도에 의거하여 비교가 가능하다. 
이 과정중 새로 알게된 함수 중 model.fit시 callbacks인자로 callback function설정이 가능한데, loss가 증가할때 멈추는 EarlyStopping과 수시로 weight백업을 만드는 ModelCheckpoint같은 것이 제공된다.
평균 벡터 말고도 직접 Doc2Vec을 일반적으로 우리가 사용하는 방식(인스턴스화, vocab등록, train)으로  사용할 수 있다. 

[RNN을 이용한 텍스트 분류(Text Classification)]
뭐 기본적인 내용 RNN학습시 단어들이 embedding->RNN->Dense->loss_function으로 들어가는 그런 건 똑같고 이  챕터는 응용을 좀 보여주는거같음 근데 그러면서 굉장히 중요하고 유용한 Tool, 기법들이 많이 나옴
우선 Test Classification은 many-to-many에 속하며, 분류 클래스의 개수만큼 Dense layer의 크기가 결정된다. 일반적으로 pandas에서 data.isnull().values.any()이런걸로 결측값을 확인하는 것 외에도 data.info()를 통해 기본적인 state들을 확인할 수 있으며
label이 이 정상데이터(85%), 스팸(15%)처럼 불균형 할 경우 단순히 train_test_split시 정상데이터만 분리될 수 있기에 train_test_split()함수의 추가적인 stratify인자로 직접 label데이터를 지정하여 고른 분포로 분리될 수 있게 할 수 있다.
또한 text_to_sequences()로 빈도수기반 integer encoding시 단순히 빈도수가 낮다고 tokenizer instantidation시 num_words를 대충 주는게 아니라 실제 빈도수가 낮은 단어(rare_word)의 전체 데이터에서 차지하는 비율, 그리고 빈도수 계산에서 차지하는 비율을 실질적으로 확인해야 한다.
그렇지 않으면 대충 빈도수 낮다고 자른 데이터가 알고보니 train_data의 대다수여서 training이 제대로 되지 않는 상황이 발생할 수 있으니 섬세하게 데이터를 cutting해야한다.
또한 이러한 상태를 전반적으로 판단하는데에 matplotlib를 적극 활용하면 좋다. 일반적인 text result의 경우 최대값, 최소값, 평균값(pooling? ㅋ)처럼 특별한 값만 확인하지 전반적인 값을 확인할 순 없는데, 시각화도구를 사용하면 전반적인 데이터의 상태, 분포를 한눈에 파악할 수 있다.
또한 이러한 시각화는 model evaluation에서 유용한데, 데이터에 적합한 training epoch를 알 수 있다(overfitting 시점 epoch에 따라) 그리하여 추가적인 데이터의 필요성등을 확인하여 더 나은 모델로 모델을 발전시킬 수 있다! 이번 챕터는 느낀게 좀 많네ㅎㅎ
 로이터뉴스사용방법을 알 수 있고 LSTM을 이용하여 분류한다. validation에 사용하는 data와 evaluation에 사용하는 data가 같은데, 데이터의 양이 많다면 둘은 다른 데이터를 사용하는 것이 국룰이다.
 IMDB리뷰감성분류에 GRU사용하는 예시를 알 수 있는데, LSTM단순화버전에 성능 비슷해서 별거 얻을 건 없음. 그대신 ENN hidden_state, embedding_dim을 추정하는데 감을 잡기위해 한번 RNN별 사항을 정리해보겠음
 베이즈정리를 기반으로 빈도수에따라 분류하는 Naive Bayes Classifier이다. 빈도수기반이기에 CountVectorizer즉, BoW생성기를 사용하며BoW기반 러닝에 서 보다 정확한 분류가 가능하게 하기 위해 TF-IDF weight를TfidTransformer를 이용하여 반영한다. 
이때모든 단어가 독립적확률을 갖는다고 가정하는 베이즈 정리 특성 상 일부 단어가 전체 문장의 확률을 0으로 만들어버릴 수 있기에 0이 아닌 가장 작은 값 을 곱하는 라플라스 스무딩(Laplace Smoothing)사용이 가능하며 이는 나이브베이즈분 류기인 MultinomialNB의 alpha인자로 결정이 가능하다
 네이버 영화감성분류 이거 찐이다...이거 얻을거 많다..그전까지는 약간 이론상으로 partially하게 봤다면 이건 실전이다..전처리 과정 일단 많고 데이터 하나 삭제하고 정제하는데에 중요한 데이터일 수 있기에 num_words정할때나 padding_size정할때, null값유무확인 분포, 비율 다 확인하며 최적의 loss확인하고...이거 찐이다
그 외에는 비슷비슷함ㅋㅋ 다만 실전예제같은 느낌으로 굉장히 호흡이 길고 세심한 작업이 이루어짐에 감탄했음. 전반적으로의 전처리 과정을 조금 정리하면 아래와 같음
데이터 정제(중복샘플제거)->label의 분포가 균일한지 확인->null값의 존재 유무->regex이용 필터링->null값의 존재유무->(테스트데이터에도 전처리 적용)->토큰화 with 불용어 제거->정수 인코딩(num_words전 비중확인)->빈 샘플 제거->패딩(패딩하며 잘려지는 데이터 분포 확인) 까지가 전처리ㅋㅋ
 Counter을 사용해서 word를 세면 most_common같은 유용한 메서드 사용 가능하다. 그리고 웬만한 오류는 colab이어서 그런거고 오류나면 처음부터 다시 싹 컴파일해봐라!
Bidirectional LSTM처럼 Bidirectional을 사용하려면 model.add(Bidirectional(LSTM(~처럼 사용하면 된다!

SimpleRNNL embedding_dim=32, hidden_units=32 / LSTM: embedding_dim=128, hidden_units=128 / GRU(many-to-one): embeddding_dim=100, hidden_units=128 / LSTM(네이버리뷰감성): embedding_dim=100, hidden_units=128 / (대체적으로 embedding_dim과 hidden_units이 비슷한 경향이 있고, 2^라는 경향이 있는듯)

[NLP를 위한 합성곱 신경망(Convolution Neural Network)]
spatial structure정보를 보존하기 위한 것으로, 이미지(높이, 너비, 채널) 3D Tensor을 입력으로 kernel(filter) matrix로 이미지를 (좌상->우하)로 훑으며 겹쳐지는 부분의 곱을 합한 값이다.
한번의 훑는 과정을 step이라 하고, 커널의 이동범위를 stride라 하며, 그 결과를 feature map이라 하며 이 칸수는 step의 횟수와 동일하다. 크기유지를 위해 padding을 사용하며, 보통 conv연산전에 미리 해둔다.
kernel이 가중치로서 학습되며, bias역시 추가가 가능한데 이는 feature map에 더해지며 단 하나의 값만 존재한다. 입력의 크기, 커널의 크기, stride로 feature map크기계산이 가능한데 사진의 식을 참고하자.
다수의 채널을 가진 입력의 경우 커널도 같은 채널크기를 가진다.
 자연어 처리를 위해선 embedding vector가 필요한데, 너비가 고정되어 커널의 높이설정만으로 위아래로 움직이는 1D convolution으로 embedding vector의 획득이 가능하다. 
풀링은 1D convolution에서 embedding vector의 최댓값을 꺼내온다. Keras구현시 이진분류이지만 softmax를 사용하여 2개의 출력층 뉴런을 가지며, 여러 커널로 이미지 conv를 통해 얻은 값들을 concatenate하여 출력층의 입력으로 사용한다.
 1D CNN으로 text분류시 hyperparameter는 embedding_dim, dropout_ratio, num_filters, kernel_size, hidden_units정도 되고 Conv뒤 Pooling을 거치면 Scalar가 되기에 다음 input_dim고려는 num_filters에만 의존하면 된다는 거 정도 괜춘.
 Multi-Kernel CNN사용시 functional API기반 모델이므로 for문으로 쉽게 모델을 표현할 수 있다는 장점이 있고, 그 결과들을 list에 append해뒀다가 Concatenate()로 합쳐서 사용하는 방법에 유의. Concatenate도 일반적인 functional 버전 model.add()처럼 z=Concatenate()(conv_blocks)처럼 하면 된다.(괄호로)
 의도 분류하는데에 일단 이진분류가 아니라 label처리도 해야하는데, preprocessing.LabelEncoder로 쉽게 가능하고, 이를 기반으로 train & test의 label데이터를 integer encoding함. 그리고 마찬가지로 intent_train을 tokenizer에 등록해서 integer encoding하고
padding하고 validation_data split하고 사전 훈련된 GloVe를 가져와 저장하고 vocab에 해당하는 embedding_vector를 모아 따로 matrix를 만들었음. 그리고 모델을 설계할때 Embedding에서 weight로 그 행렬을 설정하고, trainable=False로 하여 GloVe embedding_vector를 고정적으로 사용함.
그렇게 Multi-Conv 사용하니까 정확도가 99프로 나옴. 그리고 유의해야하는건 one-hot encoding을 수행했고 그로인해 softmax를 사용하는데, intent category가 여러개라 그 확률들을 나타내기위해서 label들을 다 one-hot encoding함. 그리고 train데이터들은 차피 모델지나며 softmax지날테니 상관없고.
그리고 개인적으로 fitting이전에 웬만한 데이터들의 shape들을 확인하는 습관을 들으면 정말로 좋을거같고 나중에 model evaluation, summary시에 plot을 accuracy뿐이 아닌 loss도 같이 표시해서 최대한 유용한 정보를 많이 얻는 습관도 굉장히 좋아보임.
 Character Embedding은 비슷한 유사한 단어에 대한 사람의 이해능력을 흉내내는 알고리즘으로 1D CNN을 통해서 마찬가지로 문자단위로 분리하고 문자를 Embedding하고 1D CNN(kernel_size=4 2개, kernel_size=3 2개, kernel_size=2 2개)를 통과시켜 6개의 Scalar를 얻고, 이를 concatenate하여 Character-level representation을 만든다. 이것이 단어의 vector이다.
BiLSTM의 구현은 many-to-one구조로 유사하게 순방향LSTM의 마지막 timestep의 hidden_state와 역방향LSTM의 첫번째 timestep의 hidden_state를 concatenate하여 이 벡터를 Character-level representation으로 해당 단어의 벡터로 사용한다.
이러한 문자 임베딩은 워드 임베딩의 대체제 혹은 문자 임베딩과 워드 임베딩을 concatenate하여 또다른 신경망의 입력으로 사용되기도 한다.

IMDB: embedding_dim=256, num_filters=256, kernel_size=3 / 스팸메일: embedding_dim=32, num_filters=32, kernel_size=5 / 네이버영화리뷰: embedding_dim=128, num_filters=128, kernel_size=3~5 / 

[태깅작업(Tagging Task)]
태깅작업으로는 Named Entity Recognition, Part-of-Speech Tagging이 있으며, pair을 가진다는 특징이 있다. 또한 입력 시퀀스에 쌍을 이루는 레이블 시퀀스를 구성하는 작업을 Sequence Labeling Task라고 하며, 태깅의 경우 many-to-many구조이기에 RNN의 return_seqeunces=True로 하여 처리한다.
zip()함수에 *이 사용되면 unzip을 의미하며, BiLSTM으로 Pos_tagger을 만들 때 TimeDistributed()를 이용해 weight update를 향상시킬 수 있으며, one-hot encoding이 아닌 integer encoding과 sparse_categorical_crossentropy를 사용하여 순서관계를 보다 잘 나타낼 수 있다.
결론적으로 굉장히 단순한 구조로 문자 임베딩도 아니고 단어 임베딩으로 pos tagging을 진행하지만 BiLSTM을 사용하므로서 문맥정보를 반영하여 추론하게 하여 보다 성능이 좋은 pos_tagger를 만들 수 있게 한다.
 역시 사람은 반복학습이라고 전에 사실 제대로 이해안됐었는데 이번에 제대로 이해했고 일단 BIO표현을 이해하는 것이 중요해 보임. 전처리는 지금까지 해온 전처리와 같지만 개체명 인식에서 중요한 것은 Sequence Labeling Task인 듯 함.
우선 작업 처리하기 편하게 단어: Tag식으로 전처리 한다음에 zip(*)으로 Sequence Labeling task를 통하여 실제 훈련에 사용한 X_data와 y_data를 제작하는 역활을 함. 그다음 마찬가지로 padding하고 texts_to_sequences하는데, BIO특성상 OOV token이 있을 수 있으니 "PAD"로 바꾼다든지 어느정도의 데이터프레임의 이해가 뒷받침되어야 할듯
무튼 원-핫 인코딩을 사용했는데 아직 정수 인코딩 sparse학습과의 사용타이밍 구분은 잘 모르겠고, many-to-many구조를 사용하기에 각 timestep마다 출력층을 사용함. 이때 단순히 return_sequences만 True로 설정하는게아니라
TimeDistributed를 Embedding layer에 사용하는데, 이는 내부적으로 각 timestep별로도 weight update를 진행하는거임. 근데 many-to-many구조의 경우 기 timestep마다 사용할 정보들이 담겨있을테니 보다 정확한 데이터를 위해 사용함.
그리고 이러한 tagging작업에선 솔직히 BIO표기 시 trash값 O가 많기에 이를 연산에서 배제시키도록 하는 인자 mask_zero=True를 Embedding에서 사용할 수 있음. 그리고 모든 training이 종료 된 후 기존 evaluation 기준인 accuracy의 경우
O Tag가 너무 많다보니 prediction을 다 O으로 해도 정확도가 75%정도 나와서 training의 혼란을 야기할 수 있기에 정답이라고 예측한 것 중 실제인 것, 특정 개체중에서 실제인 것을 의미하는 precision과 recall의 조화평균값을 사용하는데, 이는 f1-score라고 부르며 seqeval 패키지에서 사용가능함! 아이고 호흡 길다
 CRF(Conditional Random Field)는 기존 BiLSTM모델에서 BIO제약사항들을 고려하지 않아 비효율적이라는 점을 개선하기 위한 것으로 LSTM의 활성화 함수 결과를 입력으로 사용하여 BIO의 제약사항들을 학습할 수있게 한다. 다만 keras-crf가 one-hot vector를 지원하지 않아 Integer encoding을 사용한다.
흐름을 굉장히 귀찮기에 대충 정리하면 이라고 하려했는데 이미 CRF까지 되어있고만 과거의 성실한 나 칭찬해! 별거없음 .마지막에는 어느 모델이 실제 개체명 태깅에 효율적일지 이것저것 해보는 걸로 LSTM+CNN, BiLSTM+CNN+CRF, BiLSTM+BiLSTM+CRF 이런느낌으로 F1-score측정해보는게 다임

-------------------------심화과정-------------------------------------

[서브워드 토크나이저(Subword Tokenizer)]_원래 여기부터 귀찮아서 안하려다가 굉장히 유용한 모듈들의 간단한 사용법을 알려주는거같아 사용법과 개념을 간단히 lookup table처럼 사용할 목적으로 정리.
 1) 바이트 페어 인코딩 
뛰어난 모델이더라도 vocab에 없는 단어가 들어오면 까다로워지는 문제 OOV(Out-Of Vocabulary)문제를 해결하기 위한 여러 기법중 대표적인 것이 서브워드 분리(Subword Segmentation)으로 완화하는 것이다.
이러한 작업을 하는토크나이저가 서브워드 토크나이저고, 이 서브워드 분리 알고리즘으로 대표적인 것이 BPE(Byte Pair Encoding)기법이다. 실제 이 기법은 반복되는 것에 대한 치환에 기반을 두지만, 자연어처리에서는 글자(character)에서 점차 단어집합(vocabulary)를 만들어내는 Bottom Up방식의 접근을 의미한다.
훈련 데이터의 단어를 chracters 혹은 unicode단위로 vocabulary를 만들고, 빈도수 기반으로 유니그램들을 merge하는 것이다. (각 단어의 frequency를 각 char에 할당하고, 입력된 sentence에 대하여 모든 pair가능한 경우 중 가장 char의 frequency합이 큰 것을 기준으로.)
이 외에도 위의 BPE(Bytes Pair Encoding)을 참고하여 제작된 Wordpiece Tokenizer나 unigram Language Model Tokenizer 등의 서브워드 분리 알고리즘이 존재한다.
 간단하게 설명하면 WordPiece Tokenizer은 기존의 BPE가 빈도수를 기반으로 pair을 merge한 것과 달리, 병합 시 corpus의 우도(Likelihood_가능성)를 높이는 pair을 찾아 merge한다. (참고로 띄어쓰기는 _로 치환하여 구분자 space와 구분하고, BERT훈련에 이 알고리즘이 사용되기도 했다.)
Unigram Language Model Tokenizer은 각 서브워드들을 vocab에서 제거 시 corpus의 우도(Likelihood)가 감소되는 정도 즉, 손실(loss)를 계산하여 최악의 영향을 미치는 10~20%의 토큰을 제거하여 vocab_size를 조정하며 vocab을 만드는 알고리즘이다.

 2) 센텐스피스(SentencePiece): import sentencepiece as spm
BPE와 같은 여러 서브워드 토크나이징 알고리즘을 내장한 패키지이다. 토큰화 없이 전처리를 하지않은 데이터에 바로 사용된다. 
spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=500 --model_type=bpe --max_sentence_length=9999') #input데이터를 size5000의 BPE모델을 만든다.(이름: imdb) 즉, 데이터를 기반으로 서브워드들을 학습한다.(imdb.model, imdb.vocab파일 생성)
sp=spm.SentencePieceProcessor()#실제 사용하기위한 프로세서 인스턴스화.
sp.load("imdb.model")#학습된 모델을 로드
위의 과정을 기본으로 사용 준비를 마치며, encode_as_pieces(), encode_as_ids(), GetPieceSize(), IdToPiece(), PieceToId(), DecodeIds(), DecodePieces, encode()등의 기능을 지원한다.

 3) 서브워드텍스트인코더(SubwordTextEncoder): import tensorflow_datasets as tfds
BPE와 유사한 Wordpiece Model을 채틱한 텐서플로우 서브워드 토크나이저이다.
tokenizer=tfds.features.text.SubwordTextEncoder.build_from_corpus(data, target_vocab_size=2**13)#과 같이 데이터를 학습시킬 수 있다. 일반적인 keras의 tokenizer처럼 바로 인스턴스화된다.
tokenizer의 함수 subwords[index], encode(data), decode(encoded_data), vocab_size등의 기능을 사용할 수  있다.

 4) 허깅페이스 토크나이저(Huggingface Tokenizer): from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer
BERT의 WordPiece Tokenizer을 허깅페이스 회사에서 만든 짝퉁이다. 위의 임포트 다른거는 bytelevel, original, sentencepiece와 호환되는 토크나이저종류들이다.
tokenizer=BertWordPieceTokenizer()# 인스턴스화하고
tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency)#와 같이 학습시킨다. 
tokenizer.save_model()#로 vocab을 저장할 수 있다.(수직데이터이기에 pd.read_fwf사용 요망)
기능으로는 encoded=tokenizer.encode(sentence)로 문장을 인코드하면 encoded.tokens, encoded.ids로 해당 sentence의 변환결과 접근이 가능하고, 디코드는 tokenizer.decode()를 사용하면 된다.

[RNN을 이용한 인코더-디코더]
 1) 시퀀스-투-시퀀스(seq2seq)
RNN2개를 인코더, 디코더로 서로 연결하여 사용하는 걸로, 입력문장과 출력 문장의 길이가 다른 경우 사용한다. Encoder는 context vector을 출력하고, 이를 디코더가 받아 번역된 단어를 한 개씩 순차적으로 출력한다.
디코더는 <SOS>~<EOS>까지 예측하는데, 훈련시 하나의 cell이 잘못 예측하면 뒤의 셀들도 연쇄적으로 잘못 학습된다는 것을 방지하기 위해 decoder의 input을 주어 셀하나하나 훈련시키는 teacher forcing을 사용한다. 고로 실제 테스트시에는 decorder_input을 따로 받지 않고 전 state를 받게 설계한다.(<EOS>필요없음. 어차피 전꺼받음)
디코더의 각 timestep별 출력은 Dense, Softmax를 거쳐 vocab확률벡터가 출력되게끔 한다. 이러한 Character-Level Machine Translation을 위해서는 두개 이상의 언어가 병렬적으로 구성된 코퍼스, parallel corpus가 필요하다.

 2) Word-Level 번역기 만들기
마찬가지로 urllib3.PoolManager()의 request를 사용하여 데이터 다운을 보다 쉽게 하며, char-level이 아닌 word-level이기에 Tokenizer을 사용하여 vocab을 생성한다. 
padding은 따로 Masking층을 두어 고려를 안할것이기에 별도의 padding_size를 설정하지는 않았으며, 전반적인 machine구조에 초점을 맞추었으면 한다.
훈련시 Encoder: input->embedding->masking->lstm->state, Decoder: input->embedding->masking->lstm(with Encoder state)->dense->output이며, Model생성 시 ([encoder_inputs, decoder_inputs], decoder_outputs)꼴로 넣는다. 즉 결과를 states_value꼴로 받는다.
작동시 Encoder: 동일, Decoder: [decoder_state_input_h, decoder_state_input_c]꼴의 input->embedding->lstm(with Encoder state)->dense->output이며, Model생성 시 ([decoder_inputs]+decoder_states_inputs, [decoder_outputs2]+decoder_states2)꼴로 넣는다. 즉 결과를 output_tokens, h, c꼴로 받는다.

 3) BLEU score(Bilingual Evaluation Understudy Score)
PPL(Perplecity)로 Language Model의 성능을 평가했었지만, 번역의 성능을 직접적으로 반영할 수 없는 방식이기에 자연어처리에서 대표적으로 BLEU를 사용한다. n-gram에 의거, 사람번역과 기계번역의 유사도를 측정하는 것으로 언어에구애받지않고 빠르게 계산이 가능하다.
단어의 빈도수 기반에 중복을 제거하여 Modified Unigram Precision을 사용하고, 여기에서 순서를 더 반영하기 위해 각 n-gram들에 가중치곱으로 모든 n-gram을 조합하여 사용하여 BLEU를 사용하는데, 짧은 단어의 경우의 모순성, 너무 긴 단어에 대하여 불필요한 연산등을 종합적으로 고려하여
기계번역과 사람번역에 길이 차이에따라 값에 패널티를 부과하는 Brevity Penalty를 BLEU에 적용하여 향상시켰고, 이는 nltk.translate.bleu_score에서 사용이 가능하다.

[어텐션 메커니즘(Attention Mechanism)]
 1) 어텐션 메커니즘
seq2seq의 context vector의 정보손실, RNN의 고전적인 Vanishing gradient를 보완하기 위한 것으로, Attention function을 이용하여 디코더에서 출력단어 예측 시 인코더의 전체 입력 문장을 현재 decoder의 timestep에 연관성을 고려하여 반영하는 것이다.
Current Decoder's hidden_state를 Encoder의 모든 timestep hidden_state와 중간수식연산(dot-product)한 값을 softmax를 거치게 해 현재 timestep에서 decoder cell에 미치는 확률인 Attention Distribution(의 각 값을 Attention Weight라고 함.)을
decoder의 hidden_state와 가중합(곱하여 합계)한 값을 Attention Value(Context Vector_참고로 seq2seq의 context vector와 다른것이다)이라고 하고, 이를 decoder의 hidden_state와 concatenate하고 tanh를 거쳐 다음 cell의 입력으로 넣는다.
즉, decoder의 cell은 hidden_state, cell_state외에도 이 Attention_value까지 총 3개의 입력을 받는것이다.

 2) 바다나우 어텐션
루웅 어텐션과 달리 decoder t-1 hidden_state를 사용하는 점이 다르며, 학습가능한 가중치 행렬3개를 이용하는데 Encoder 각 timestep에 하나, Decoder t-1 hidden_state에 하나씩 곱해 더한 후 tanh에 넣고 여기에 하나 더 곱하여 Attention Score벡터를 엊는다.
이 Attention Score에 softmax를 적용해 Attention Distribution을 구해 이 Attention Weight들을 Decoder t-1 hidden_state에 가중합 하여 Context Vector를 구하고, 이를 다음 디코더 시점의 wor embedding vector에 concatenate하여 사용한다.

 3) 양방향 LSTM과 어텐션 알고리즘
class API를 사용해 구현했는데, init으로 Encoder hidden_state, Decoder t-1 hidden_state, tanh뒤 곱해질 행렬 총3개를 init하고, call시에 해당 행렬들과 인자로 들어올 lstm, hidden_state로 연산 후 context_vector, attention_weights를 반환하면 된다.
추가적으로 알아야하는건 BiLSTM을 사용했기에 attention입력으로 사용되는 state는 forward와 backward를 concatenate해서 보내야한다는거정도.

[트랜스포머(Transformer)]
 1) 트랜스 포머(Transformer)
요...요약따위..없어...그림...전체 그림이나 봐..

 2)