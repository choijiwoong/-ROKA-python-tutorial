[텍스트 전처리]
텍스트 전처리 과정에서 토큰화를 통해 corpus를 token단위로 나눌 수 있는데, 기준이 단어면 word tokenization, 문장이면 sentence segmentation이라고 한다.

 NLTK의 word_tokenize와 WordPunctTokenizer, Keras의 text_to_word_sequence를 통해 word_tokenize처리가 가능하지만 각기 구두점, 아포스트로피 등 처리 규칙이 약간씩 다르다.
표준으로 사용되는 토큰화 방법중 하나인 Penn Treebank Tokenization는 nltk.tokenize에서 제공해주고 있다.

 sentence tokenization역시 nltk.tokenize의 sent_tokenize로 사용이 가능하다. 한국어 sentence segmentation의 경우 KSS(Korean Sentence Splitter)가 주로 사용된다.
한국어의 경우 교착어 특성 상 불규칠 활용, 규칙활용, 자립형태소, 의존형태소 등 고려해야 할 것이 많다. 고로 품사 태깅(Part-of-speech tagging)의 경우도 영어의 경우
nltk.tag의 pos_tag를 이용하여 Penn Treebank POS Tags기준을 사용할 수 있지만, 한국어의 경우 별도로 KoNLPy라는 파이썬 패키지를 사용하여 Okt(Open Korea Text), Mecab, Komoran, Hannanum, KKma
등의 형태소 분석기를 따로 사용해야 한다.

위의 토큰화 작업 전,후로 노이즈데이터를 제거하는 정제(Cleaning) 및 표현만 다른 단어로 통합시키는 정규화(Normalization)가 함께하는데, 이과정에서 정규식이 잘 사용된다.
정규화(Normalization)의 기법 중 단어의 개수를 줄이기 위해 뿌리 사전단어를 찾아가는 표제어 추출(lemmatization)과 어간을 어림짐작하는 어간추출(stemming)이 있다.

노이즈 데이터를 제거하는 정제(Cleaning)을 위해선 큰 의미가 없는 단어를 제거하는 작업도 필요한데 이를 특별히 불용어(stopword)라고 부르며 영어의 경우 nltk.corpus의 stopwords에서 확인이 가능하다.

텍스트 전처리에 사용되는 유용한 도구인 regular expression. re를 알아본다.

Integer Encoding은 각종 정제, 정규화 과정과 함께 연산효율을 높이기 위해 사용하는 것으로, 보통 단어 등장 빈도수를 기준으로 index를 부여하여 mapping하며, mapping standard(매핑기준)에 해당하지 않는 항목에 대해선 Out-Of-Vocabulary로 따로 indexing한다.
이때 등장 빈도수를 직접 알아낼 수도 있지만, collections.Counter, FreqDist, enumerate, keras의 Tokenizer등을 사용하여 알아내는 편리한 방법도 있다.(feat. most_common, sum(~,[]), np.hstack())

특정값으로 shape를 조정하는 것이 padding이며, keras의 pad_sequences를 사용하여 편리하게 padding이 가능하다.

문자를 숫자로 변환하는 Integer Encoding에서 더 구체적으로 계산에 편리한 벡터로 바꾸는 것이 One-Hot Encoding이며, keras의 to_categorial로 변환이 가능하다. 다만 단어간의 연관성을 표현못한다는 단점이 있어 이를 보완하기 위해 다차원 벡터로 나타내는 Word2Vec등의 방법이 존재한다.

Supervised Learning은 기본적으로 데이터를 학습을 위해 X_train, y_train, X_test, y_test 4가지의 데이터로 분리하는데, 이때 slice를 사용할 수도 있지만, sklean의 train_test_split을 이용하여 특정 개수 혹은 비율로 분리가 가능하다.

유용한 한국어 전처리 패키지로 PyKoSpacing(오토 띄어쓰기), Py-Hanspell(맞춤법검사), soynlp(cohsion probability & branching entropy로 단어점수표 이용 토크나이저를 지원, L tokenizer로 어절분리가능, normalizer로 반복문자정제가능), ckonlpy(customized konlpy_사용자 단어 추가가능)이 있다.

[언어 모델]
자연스러운 단어 시퀀스를 찾아내는 모델로 적절성은 Machine Translation, Spell Correction, Speech Recognition을 이용하여 판단하며, 내부적으로 conditional probability를 이용한다.
Count기반인 Statistical Language Model의 희소문제를 해결하기 위해 count확률을 높이는 N-gram Language Model이 있지만, 근본적인 문제 해결의 한계와 n의 크기에 따른 trade-off로 인해 Neural Netword Based Language Model이 선호된다.
이때 두 모델의 성능을 비교하기 위해 Perplexity(PPL)을 사용하는데, 값이 낮을수록 좋으며 측정 시점에 모델이 고민중인 선택지의 개수를 brancing factor라고 한다.

[카운트 기반의 단어]
크게 Local(Discrete) Representation_해당단어만집중, Distributed(Continuous) Representation_앞뒤단어도 참고로 나뉘며, Local Representation중 단어순서와 상관없이 frequency만 집중하는 Bag-of-Words(BoW)가 있다.
이는 sklearn의 CountVectorizer에서 지원하며(띄어쓰기 기준이기에 영어만. 한국어 지X) BoW는 frequency만 보아 단어의 중요도를 판단하겠다는 거라 불용어제거하면 성능이 좋아지며 CountVectorizer의 생성자인수 stop_words에 지정하여 불용어를 설정한다.
 서로 다른 문서들의 BoW를 비교할 수 있게 결합한 것이 Document Term Matrix이며, 대부분값이 0인 벡터로 인한 sparse representation과 빈도수 기반 접근이기에 the와같이 의미없는 단어에 가중치를 준다는 문제를 해결하기 위해
DTM에 TF(d,t)_문서d에서의 단어t의 등장횟수 - IDF(d,t)_df(t)_단어t가 등장한 문서수에 반비례하는 수, 즉 TF-IDF가중치를 추가하여 나타낸다. TF-IDF는 모든 문서에서 자주등장하면 중요도를 낮게, 특정 문서에서 자주 등장하면 중요도를 높게 판단한다.
직접 구현하게되면 로그 특성으로 가중치가 0이되는 문제가 발생할 수 있기에, 조정된 식을 이용하여 TF-IDF를 자동계산해주는 사이킷런의 TfidfVectorizer가 있다.