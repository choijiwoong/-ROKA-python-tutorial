"""
[정제와 정규화]
코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 tokenization이며, 토큰화 정 후로 텍스트 데이터를 용도에 맞게 정제 및 정규화한다.
정제cleaning은 갖고 있는 코퍼스로부터 노이즈 데이터를 제거하는 것이 목적이며(토큰화 후 남아있는 노이즈를 제거하기 위해서기도 함),
정규화 normalization은 표현방법이 다른 단어들을 통합시켜서 같은 단어로 만들어 주는 것이 목적이다.

[규칙에 기반한 표기가 다른 단어들의 통합]
USA와 US, uh-huh와 uhhuh같이 의미가 같은 것은 하나의 단어로 정규화가 가능하다. 이처럼 표기가 다른 단어들을 통합하는 방법은 어간 추출(stemming), 표제어 추출(lemmatization)이라고 한다.

[대소문자 통합]
영어권에서 단어의 개수를 줄이기 위해서도 있지만, 사용자 검색을 편하게 하는 장점도 있다.
물론 예외상황들에 따라 대처가 다르지만 때론 예외를 고려하지 않고 소문자화시키는 것이 실용적인 경우가 되기도 한다는 것이다.

[불필요한 단어의 제거]
불필요한 데이터 역시 noise data로 분류된다. 여러 데이터를 분석해야하는데 등장 빈도가 너무 작다면 분류에 도움이 되지 않을 것이기에 등장 빈도가 적은 단어도 noise 데이터로 포함되며,
영어권 언어에서는 길이가 짧은 단어들(it, at, to, on, in, by)은 대부분 불용어에 해당하기에 어느정도 효과를 볼 수 있다.
"""
import re
text="I was wondering if anyone out there could enlighten me on this car."

shortword=re.compile(r'\W*\b\w{1,2}\b')#단어 경계 사이에 1,2글자
print(shortword.sub('', text))#.sub는 RE가 일치하는 모든 부분 문자열을 찾고 다른 문자열로 대체한다.
#잃어도 되는 데이터인지 고민이 당연히 필요하다.

#Regular Expression으로 노이즈데이터를 위와같이 제거할 수 있는 경우가 많다. html의 태그들이라던지 뉴스의 업로드 시간이라던지
#코퍼스 내에 계속등장하는 글자들을 규칙에 기반하여 한번에 제거하는 것은 매우 유용하다.
